{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tk541/mambaforge/envs/mr/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from model import DQN\n",
    "from trainer import DQNTrainer\n",
    "from exploration import EpsilonGreedyExploration, quadratic_decay_schedule\n",
    "from buffer import ReplayBuffer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "env_name = \"ALE/MontezumaRevenge-v5\"\n",
    "max_steps = 1000000\n",
    "initial_epsilon = 1.0\n",
    "batch_size = 4\n",
    "\n",
    "params = dict(\n",
    "    env_name=env_name, \n",
    "    Model=DQN,\n",
    "    model_params=dict(\n",
    "        conv_feats=256,\n",
    "        hidden_dim = 128,\n",
    "        n_layers = 12,\n",
    "        Activation = nn.ReLU,\n",
    "        Norm = nn.LayerNorm,\n",
    "    ),\n",
    "    exploration=EpsilonGreedyExploration(\n",
    "        epsilon=initial_epsilon,\n",
    "        decay_schedule=quadratic_decay_schedule(\n",
    "            initial_epsilon=initial_epsilon,\n",
    "            final_epsilon=0.3,\n",
    "            max_step=max_steps,\n",
    "        )\n",
    "    ), \n",
    "    Buffer=ReplayBuffer, \n",
    "    buffer_params=dict(\n",
    "        batch_size=batch_size,\n",
    "        min_size=320,\n",
    "        max_size=max_steps // 50,\n",
    "    ),\n",
    "    discount_rate=0.9,\n",
    "    loss_fn=torch.nn.SmoothL1Loss(beta=1.0),\n",
    "    Optim=torch.optim.RMSprop,\n",
    "    lr=1e-5,\n",
    "    time_step_reward=-3e-3,\n",
    "    network_frozen_steps=1000,\n",
    "    seed = 42,\n",
    "    max_steps = max_steps,\n",
    "    debug = False\n",
    ")\n",
    "\n",
    "trainer = DQNTrainer(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1000000 [00:00<?, ?step/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tk541/mambaforge/envs/mr/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "Training:   2%|▏         | 24694/1000000 [16:42<11:14:14, 24.11step/s, value_loss=4.92e-15]"
     ]
    }
   ],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "montezuma-revenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
